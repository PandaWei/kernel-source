From: Hannes Reinecke <hare@suse.de>
Date: Wed, 5 Sep 2018 08:11:23 +0200
Subject: [PATCH] nvme: kABI-compliant version of nvmf_fail_nonready_command()
Patch-Mainline: never, kABI fix for SLE12 SP3
References: bsc#1102486

Introduce __nvme_fail_nonready_command() to keep the original function
for kABI compliance.

Signed-off-by: Hannes Reinecke <hare@suse.com>
---
 drivers/nvme/host/fabrics.c | 13 +++++++++++--
 drivers/nvme/host/fabrics.h |  5 +++--
 drivers/nvme/host/fc.c      |  2 +-
 drivers/nvme/host/rdma.c    |  2 +-
 drivers/nvme/target/loop.c  |  2 +-
 5 files changed, 17 insertions(+), 7 deletions(-)

diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 99140d81b86c..d04dcabd38f9 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -543,8 +543,8 @@ static struct nvmf_transport_ops *nvmf_lookup_transport(
  * Note: commands used to initialize the controller will be marked for failfast.
  * Note: nvme cli/ioctl commands are marked for failfast.
  */
-int nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
-		struct request *rq)
+int __nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
+				 struct request *rq)
 {
 	if (ctrl->state != NVME_CTRL_DELETING &&
 	    ctrl->state != NVME_CTRL_DEAD &&
@@ -553,6 +553,15 @@ int nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
 	nvme_req(rq)->status = NVME_SC_ABORT_REQ;
 	return BLK_MQ_RQ_QUEUE_ERROR;
 }
+EXPORT_SYMBOL_GPL(__nvmf_fail_nonready_command);
+
+int nvmf_fail_nonready_command(struct request *rq)
+{
+	if (!blk_noretry_request(rq))
+		return BLK_MQ_RQ_QUEUE_BUSY; /* try again later */
+	nvme_req(rq)->status = NVME_SC_ABORT_REQ;
+	return BLK_MQ_RQ_QUEUE_ERROR;
+}
 EXPORT_SYMBOL_GPL(nvmf_fail_nonready_command);
 
 bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
diff --git a/drivers/nvme/host/fabrics.h b/drivers/nvme/host/fabrics.h
index 193c4d1983f2..e4f9f6a5d569 100644
--- a/drivers/nvme/host/fabrics.h
+++ b/drivers/nvme/host/fabrics.h
@@ -164,10 +164,11 @@ int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size);
 bool nvmf_should_reconnect(struct nvme_ctrl *ctrl);
 int nvmf_check_if_ready(struct nvme_ctrl *ctrl,
 		struct request *rq, bool queue_live, bool is_connected);
-int nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
-		struct request *rq);
+int nvmf_fail_nonready_command(struct request *rq);
 bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live);
+int __nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
+		struct request *rq);
 
 static inline bool nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
 		bool queue_live)
diff --git a/drivers/nvme/host/fc.c b/drivers/nvme/host/fc.c
index fccbb49d5983..2289ef193e4b 100644
--- a/drivers/nvme/host/fc.c
+++ b/drivers/nvme/host/fc.c
@@ -2316,7 +2316,7 @@ nvme_fc_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE ||
 	    !nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
-		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+		return __nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
 
 	ret = nvme_setup_cmd(ns, rq, sqe);
 	if (ret)
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 45329309e3d7..5eb95faa4ee5 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -1454,7 +1454,7 @@ static int nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	WARN_ON_ONCE(rq->tag < 0);
 
 	if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
-		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+		return __nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
 
 	dev = queue->device->dev;
 	ib_dma_sync_single_for_cpu(dev, sqe->dma,
diff --git a/drivers/nvme/target/loop.c b/drivers/nvme/target/loop.c
index d61424857292..e5d527170180 100644
--- a/drivers/nvme/target/loop.c
+++ b/drivers/nvme/target/loop.c
@@ -175,7 +175,7 @@ static int nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,
 	int ret;
 
 	if (!nvmf_check_ready(&queue->ctrl->ctrl, req, queue_ready))
-		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
+		return __nvmf_fail_nonready_command(&queue->ctrl->ctrl, req);
 
 	ret = nvme_setup_cmd(ns, req, &iod->cmd);
 	if (ret != BLK_MQ_RQ_QUEUE_OK)
-- 
2.16.4

