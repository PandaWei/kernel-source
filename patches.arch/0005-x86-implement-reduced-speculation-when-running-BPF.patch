From: Dave Hansen <dave.hansen@linux.intel.com>
Date: Wed, 9 May 2018 20:54:37 +0200
Subject: x86: implement reduced speculation when running BPF
Patch-mainline: Not yet, work in progress
References: bsc#1087082 CVE-2018-3639

Enable the SPEC_CTRL_RDS feature when running BPF code.

Underneath x86_calculate_kernel_spec_ctrl(), we now check the
per-cpu bpf_prog_ran counter.  If the counter is elevated, we
need to set the SPEC_CTRL_RDS bit.

We also add MSR writes (via x86_sync_spec_ctrl()) to:

	cpu_enter_reduced_memory_speculation() and
	cpu_leave_reduced_memory_speculation()

I'm not super happy that x86_sync_spec_ctrl() does an
unconditional MSR write.  But, they should be infrequent since
they only happen twice per timeout period.

Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Andi Kleen <ak@linux.intel.com>
Cc: Tim Chen <tim.c.chen@linux.intel.com>
Signed-off-by: Jiri Kosina <jkosina@suse.cz>
Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 arch/x86/Kconfig                 |  4 ++++
 arch/x86/include/asm/rmspec.h    | 24 ++++++++++++++++++++++++
 arch/x86/include/asm/spec-ctrl.h |  3 +++
 arch/x86/kernel/cpu/bugs.c       | 37 ++++++++++++++++++++++++++++++++++++-
 include/linux/filter.h           |  4 +---
 include/linux/nospec.h           |  2 ++
 6 files changed, 70 insertions(+), 4 deletions(-)
 create mode 100644 arch/x86/include/asm/rmspec.h

diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 01e0ed5c..b092d46d 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -75,6 +75,7 @@ config X86
 	select ARCH_SUPPORTS_ATOMIC_RMW
 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
 	select HAVE_ARCH_SECCOMP_FILTER
+	select ARCH_HAS_REDUCED_MEMORY_SPECULATION
 
 config INSTRUCTION_DECODER
 	def_bool (KPROBES || PERF_EVENTS)
@@ -174,6 +175,9 @@ config GENERIC_TIME_VSYSCALL
 	bool
 	default X86_64
 
+config ARCH_HAS_REDUCED_MEMORY_SPECULATION
+	def_bool y
+
 config ARCH_HAS_CPU_RELAX
 	def_bool y
 
diff --git a/arch/x86/include/asm/rmspec.h b/arch/x86/include/asm/rmspec.h
new file mode 100644
index 00000000..cac0189c
--- /dev/null
+++ b/arch/x86/include/asm/rmspec.h
@@ -0,0 +1,24 @@
+#ifndef _LINUX_RMSPEC_H
+#define _LINUX_RMSPEC_H
+#include <asm/msr.h>
+#include <asm/spec-ctrl.h>
+
+/*
+ * We call these when we *know* the CPU can go in/out of its
+ * "safer" reduced memory speculation mode.
+ *
+ * For BPF, x86_sync_spec_ctrl() reads the per-cpu BPF state
+ * variable and figures out the MSR value by itself.  Thus,
+ * we do not need to pass the "direction".
+ */
+static inline void cpu_enter_reduced_memory_speculation(void)
+{
+	x86_sync_spec_ctrl();
+}
+
+static inline void cpu_leave_reduced_memory_speculation(void)
+{
+	x86_sync_spec_ctrl();
+}
+
+#endif /* _LINUX_RMSPEC_H */
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 62c15a23..5a71ad2a 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -15,6 +15,9 @@
 extern void x86_spec_ctrl_set_guest(u64);
 extern void x86_spec_ctrl_restore_host(u64);
 
+/* Write a new SPEC_CTRL MSR based on current kernel state: */
+extern void x86_sync_spec_ctrl(void);
+
 /* AMD specific Speculative Store Bypass MSR data */
 extern u64 x86_amd_ls_cfg_base;
 extern u64 x86_amd_ls_cfg_rds_mask;
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index c0b804a5..491dfffa 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -11,6 +11,7 @@
 #include <linux/utsname.h>
 #include <linux/device.h>
 #include <linux/prctl.h>
+#include <linux/filter.h>
 
 #include <asm/nospec-branch.h>
 #include <asm/spec_ctrl.h>
@@ -787,10 +788,23 @@ EXPORT_SYMBOL_GPL(x86_spec_ctrl_get_default);
 
 static inline u64 intel_rds_mask(void)
 {
+	u64 mask;
+
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
 		return 0;
 
-	return rds_tif_to_spec_ctrl(current_thread_info()->flags);
+	mask = rds_tif_to_spec_ctrl(current_thread_info()->flags);
+
+	/*
+	 * BPF programs can be exploited to attack the kernel.
+	 * Leave the RDS bit on when we recently ran one.  This
+	 * bit gets cleared after a BFP program has not run on
+	 * the CPU for a while.
+	 */
+	if (get_cpu_var(bpf_prog_ran))
+		mask |= SPEC_CTRL_RDS;
+
+	return mask;
 }
 
 /*
@@ -830,6 +844,27 @@ void x86_spec_ctrl_restore_host(u64 current_spec_ctrl)
 }
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_restore_host);
 
+/*
+ * A condition that may affect the SPEC_CTRL MSR has changed.
+ * Recalculate a new value for this CPU and set it.
+ *
+ * It is not easy to optimize the wrmsrl() away unless the
+ * callers have a full understanding of all the conditions
+ * that affect the output of x86_calculate_kernel_spec_ctrl().
+ *
+ * Try not to call this too often.
+ */
+void x86_sync_spec_ctrl(void)
+{
+	u64 new_spec_ctrl = x86_calculate_kernel_spec_ctrl();
+
+	if (!boot_cpu_has(X86_FEATURE_SPEC_CTRL))
+		return;
+
+	wrmsrl(MSR_IA32_SPEC_CTRL, new_spec_ctrl);
+}
+EXPORT_SYMBOL_GPL(x86_sync_spec_ctrl);
+
 static void x86_amd_rds_enable(void)
 {
 	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_rds_mask;
diff --git a/include/linux/filter.h b/include/linux/filter.h
index 01b3d9cb..e92037f5 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -175,13 +175,11 @@ void bpf_leave_prog_deferred(const struct sk_filter *fp);
 
 static inline void bpf_enter_prog(const struct sk_filter *fp)
 {
-	int *count = &get_cpu_var(bpf_prog_ran);
-	(*count)++;
 	/*
 	 * Upon the first entry to BPF code, we need to reduce
 	 * memory speculation to mitigate attacks targeting it.
 	 */
-	if (*count == 1)
+	if (this_cpu_inc_return(bpf_prog_ran) == 1)
 		cpu_enter_reduced_memory_speculation();
 }
 
diff --git a/include/linux/nospec.h b/include/linux/nospec.h
index d23eba5a..80398eed 100644
--- a/include/linux/nospec.h
+++ b/include/linux/nospec.h
@@ -85,6 +85,8 @@ static inline void cpu_enter_reduced_memory_speculation(void)
 static inline void cpu_leave_reduced_memory_speculation(void)
 {
 }
+#else
+#include <asm/rmspec.h>
 #endif
 
 #endif /* _LINUX_NOSPEC_H */
-- 
2.12.3

