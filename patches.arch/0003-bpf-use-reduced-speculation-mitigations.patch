From: Dave Hansen <dave.hansen@linux.intel.com>
Date: Wed, 9 May 2018 20:37:46 +0200
Subject: bpf: use reduced speculation mitigations
Patch-mainline: Not yet, work in progress
References: bsc#1087082 CVE-2018-3639

The previous patches put in place the infrastructure to tell when
BPF code is running.  Now, we hook into that code to call out to
some architecture-specific code which will implement those
mitigationse

Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Andi Kleen <ak@linux.intel.com>
Cc: Tim Chen <tim.c.chen@linux.intel.com>
Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 include/linux/filter.h |  7 +++++++
 include/linux/nospec.h |  9 +++++++++
 net/core/filter.c      | 24 +++++++++++++++---------
 3 files changed, 31 insertions(+), 9 deletions(-)

diff --git a/include/linux/filter.h b/include/linux/filter.h
index f130abb9..01b3d9cb 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -11,6 +11,7 @@
 #ifdef __KERNEL__
 #include <asm/atomic.h>
 #include <linux/compat.h>
+#include <linux/nospec.h>
 #endif
 
 /*
@@ -176,6 +177,12 @@ static inline void bpf_enter_prog(const struct sk_filter *fp)
 {
 	int *count = &get_cpu_var(bpf_prog_ran);
 	(*count)++;
+	/*
+	 * Upon the first entry to BPF code, we need to reduce
+	 * memory speculation to mitigate attacks targeting it.
+	 */
+	if (*count == 1)
+		cpu_enter_reduced_memory_speculation();
 }
 
 static inline void bpf_leave_prog(const struct sk_filter *fp)
diff --git a/include/linux/nospec.h b/include/linux/nospec.h
index 0ca3b370..d23eba5a 100644
--- a/include/linux/nospec.h
+++ b/include/linux/nospec.h
@@ -78,4 +78,13 @@ int arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,
 /* Speculation control for seccomp enforced mitigation */
 void arch_seccomp_spec_mitigate(struct task_struct *task);
 
+#ifndef CONFIG_ARCH_HAS_REDUCED_MEMORY_SPECULATION
+static inline void cpu_enter_reduced_memory_speculation(void)
+{
+}
+static inline void cpu_leave_reduced_memory_speculation(void)
+{
+}
+#endif
+
 #endif /* _LINUX_NOSPEC_H */
diff --git a/net/core/filter.c b/net/core/filter.c
index ceb5dcd5..8a155fd0 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -26,6 +26,7 @@
 #include <linux/netdevice.h>
 #include <linux/if_packet.h>
 #include <linux/gfp.h>
+#include <linux/nospec.h>
 #include <net/ip.h>
 #include <net/protocol.h>
 #include <net/netlink.h>
@@ -683,17 +684,22 @@ DEFINE_PER_CPU(unsigned int, bpf_prog_ran);
 EXPORT_SYMBOL_GPL(bpf_prog_ran);
 static void bpf_done_on_this_cpu(struct work_struct *work)
 {
-	if (!this_cpu_dec_return(bpf_prog_ran))
-		return;
-
+	if (this_cpu_dec_return(bpf_prog_ran)) {
+		/*
+		 * This is unexpected.  The elevated refcount indicates
+		 * being in the *middle* of a BPF program, which should
+		 * be impossible.  They are executed inside
+		 * rcu_read_lock() where we can not sleep and where
+		 * preemption is disabled.
+		 */
+		WARN_ON_ONCE(1);
+	}
 	/*
-	 * This is unexpected.  The elevated refcount indicates
-	 * being in the *middle* of a BPF program, which should
-	 * be impossible.  They are executed inside
-	 * rcu_read_lock() where we can not sleep and where
-	 * preemption is disabled.
+	 * Unsafe BPF code is no longer running, disable mitigations.
+	 * This must be done after bpf_prog_ran because the mitigation
+	 * code looks at its state.
 	 */
-	WARN_ON_ONCE(1);
+	cpu_leave_reduced_memory_speculation();
 }
 
 DEFINE_PER_CPU(struct delayed_work, bpf_prog_delayed_work);
-- 
2.12.3

