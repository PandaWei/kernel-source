From: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Subject: x86/KVM/VMX: Add module argument for L1TF mitigation
Patch-mainline: not yet, under discussion
References: bsc#1089343 CVE-2018-3646

Add a mitigation mode parameter "vmentry_l1d_flush" for CVE-2018-3620, aka
L1 terminal fault. The valid arguments are:

 - "always"     L1D cache flush on every VMENTER.
 - "cond"       Conditional L1D cache flush, explained below
 - "never"      Disable the L1D cache flush mitigation

"cond" is trying to avoid L1D cache flushes on VMENTER if the code executed
between VMEXIT and VMENTER is considered safe, i.e. is not bringing any
interesting information into L1D which might exploited.

[ tglx: Split out from a larger patch ]

[jkosina@suse.cz: backport to older Paolo's series, drop the static keys]

Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Signed-off-by: Jiri Kosina <jkosina@suse.cz>
---
 arch/x86/kvm/vmx.c |   60 +++++++++++++++++++++++++++++++++++++++++++++--------
 1 file changed, 51 insertions(+), 9 deletions(-)

--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -68,9 +68,6 @@ static const struct x86_cpu_id vmx_cpu_i
 };
 MODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);
 
-static int __read_mostly vmentry_l1d_flush = 1;
-module_param_named(vmentry_l1d_flush, vmentry_l1d_flush, int, 0644);
-
 static bool __read_mostly enable_vpid = 1;
 module_param_named(vpid, enable_vpid, bool, 0444);
 
@@ -174,6 +171,51 @@ static int ple_window_actual_max = KVM_V
 static int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;
 module_param(ple_window_max, int, S_IRUGO);
 
+/* These MUST be in sync with vmentry_l1d_param order. */
+enum vmx_l1d_flush_state {
+	VMENTER_L1D_FLUSH_NEVER,
+	VMENTER_L1D_FLUSH_COND,
+	VMENTER_L1D_FLUSH_ALWAYS,
+};
+
+static enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush = VMENTER_L1D_FLUSH_COND;
+
+static const struct {
+	const char *option;
+	enum vmx_l1d_flush_state cmd;
+} vmentry_l1d_param[] = {
+	{"never",       VMENTER_L1D_FLUSH_NEVER},
+	{"cond",        VMENTER_L1D_FLUSH_COND},
+	{"always",      VMENTER_L1D_FLUSH_ALWAYS},
+};
+
+static int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)
+{
+	unsigned int i;
+
+	if (!s)
+		return -EINVAL;
+
+	for (i = 0; i < ARRAY_SIZE(vmentry_l1d_param); i++) {
+		if (!strcmp(s, vmentry_l1d_param[i].option)) {
+			vmentry_l1d_flush = vmentry_l1d_param[i].cmd;
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+static int vmentry_l1d_flush_get(char *s, const struct kernel_param *kp)
+{
+	return sprintf(s, "%s\n", vmentry_l1d_param[vmentry_l1d_flush].option);
+}
+static const struct kernel_param_ops vmentry_l1d_flush_ops = {
+	.set = vmentry_l1d_flush_set,
+	.get = vmentry_l1d_flush_get,
+};
+module_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, &vmentry_l1d_flush, S_IRUGO);
+
 extern const ulong vmx_return;
 
 #define NR_AUTOLOAD_MSRS 8
@@ -1985,14 +2027,14 @@ static void vmx_prepare_guest_switch(str
 	}
 
 	switch (vmentry_l1d_flush) {
-	case 0:
+	case VMENTER_L1D_FLUSH_NEVER:
 		*need_l1d_flush = false;
 		break;
-	case 1:
+	case VMENTER_L1D_FLUSH_COND:
 		/*
-		 * If vmentry_l1d_flush is 1, each vmexit handler is responsible for
-		 * setting vcpu->arch.vcpu_unconfined.  Currently this happens in the
-		 * following cases:
+		 * If vmentry_l1d_flush is VMENTER_L1D_FLUSH_COND, each vmexit handler is
+		 * responsible for setting vcpu->arch.vcpu_unconfined.Currently this happens
+		 * in the following cases:
 		 * - vmlaunch/vmresume: we do not want the cache to be cleared by a
 		 *   nested hypervisor *and* by KVM on bare metal, so we just do it
 		 *   on every nested entry.  Nested hypervisors do not bother clearing
@@ -2006,7 +2048,7 @@ static void vmx_prepare_guest_switch(str
 		 *   preempt notifier)
 		 */
 		break;
-	case 2:
+	case VMENTER_L1D_FLUSH_ALWAYS:
 	default:
 		*need_l1d_flush = true;
 		break;
