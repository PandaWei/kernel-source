From: Dave Hansen <dave.hansen@linux.intel.com>
Subject:  x86: implement reduced speculation when running BPF
Patch-mainline: Not yet, work in progress
References: bsc#1087082 CVE-2018-3639

Enable the SPEC_CTRL_RDS feature when running BPF code.

Underneath x86_calculate_kernel_spec_ctrl(), we now check the
per-cpu bpf_prog_ran counter.  If the counter is elevated, we
need to set the SPEC_CTRL_RDS bit.

We also add MSR writes (via x86_sync_spec_ctrl()) to:

	cpu_enter_reduced_memory_speculation() and
	cpu_leave_reduced_memory_speculation()

I'm not super happy that x86_sync_spec_ctrl() does an
unconditional MSR write.  But, they should be infrequent since
they only happen twice per timeout period.

Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Andi Kleen <ak@linux.intel.com>
Cc: Tim Chen <tim.c.chen@linux.intel.com>
Signed-off-by: Jiri Kosina <jkosina@suse.cz>
---
 arch/x86/Kconfig                 |    4 ++++
 arch/x86/include/asm/rmspec.h    |   24 ++++++++++++++++++++++++
 arch/x86/include/asm/spec_ctrl.h |    3 +++
 arch/x86/kernel/cpu/bugs.c       |   37 ++++++++++++++++++++++++++++++++++++-
 include/linux/filter.h           |    4 +---
 include/linux/nospec.h           |    2 ++
 6 files changed, 70 insertions(+), 4 deletions(-)
 create mode 100644 arch/x86/include/asm/rmspec.h

--- /dev/null
+++ b/arch/x86/include/asm/rmspec.h
@@ -0,0 +1,24 @@
+#ifndef _LINUX_RMSPEC_H
+#define _LINUX_RMSPEC_H
+#include <asm/msr.h>
+#include <asm/spec_ctrl.h>
+
+/*
+ * We call these when we *know* the CPU can go in/out of its
+ * "safer" reduced memory speculation mode.
+ *
+ * For BPF, x86_sync_spec_ctrl() reads the per-cpu BPF state
+ * variable and figures out the MSR value by itself.  Thus,
+ * we do not need to pass the "direction".
+ */
+static inline void cpu_enter_reduced_memory_speculation(void)
+{
+	x86_sync_spec_ctrl();
+}
+
+static inline void cpu_leave_reduced_memory_speculation(void)
+{
+	x86_sync_spec_ctrl();
+}
+
+#endif /* _LINUX_RMSPEC_H */
--- a/arch/x86/include/asm/spec_ctrl.h
+++ b/arch/x86/include/asm/spec_ctrl.h
@@ -113,6 +113,9 @@ static inline void x86_ibp_barrier(void)
 extern void x86_spec_ctrl_set_guest(u64);
 extern void x86_spec_ctrl_restore_host(u64);
 
+/* Write a new SPEC_CTRL MSR based on current kernel state: */
+extern void x86_sync_spec_ctrl(void);
+
 /* AMD specific Speculative Store Bypass MSR data */
 extern u64 x86_amd_ls_cfg_base;
 extern u64 x86_amd_ls_cfg_rds_mask;
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -28,6 +28,7 @@ config X86
 	select ARCH_HAS_FAST_MULTIPLIER
 	select ARCH_HAS_GCOV_PROFILE_ALL
 	select ARCH_HAS_PMEM_API		if X86_64
+	select ARCH_HAS_REDUCED_MEMORY_SPECULATION
 	select ARCH_HAS_MMIO_FLUSH
 	select ARCH_HAS_SG_CHAIN
 	select ARCH_HAVE_NMI_SAFE_CMPXCHG
@@ -224,6 +225,9 @@ config RWSEM_XCHGADD_ALGORITHM
 config GENERIC_CALIBRATE_DELAY
 	def_bool y
 
+config ARCH_HAS_REDUCED_MEMORY_SPECULATION
+	def_bool y
+
 config ARCH_HAS_CPU_RELAX
 	def_bool y
 
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -10,6 +10,7 @@
 #include <linux/init.h>
 #include <linux/utsname.h>
 #include <linux/cpu.h>
+#include <linux/filter.h>
 #include <linux/module.h>
 #include <linux/nospec.h>
 #include <linux/prctl.h>
@@ -152,10 +153,23 @@ EXPORT_SYMBOL_GPL(x86_spec_ctrl_get_defa
 
 static inline u64 intel_rds_mask(void)
 {
+	u64 mask;
+
 	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)
 		return 0;
 
-	return rds_tif_to_spec_ctrl(current_thread_info()->flags);
+	mask = rds_tif_to_spec_ctrl(current_thread_info()->flags);
+
+	/*
+	 * BPF programs can be exploited to attack the kernel.
+	 * Leave the RDS bit on when we recently ran one.  This
+	 * bit gets cleared after a BFP program has not run on
+	 * the CPU for a while.
+	 */
+	if (get_cpu_var(bpf_prog_ran))
+		mask |= SPEC_CTRL_RDS;
+
+	return mask;
 }
 
 /*
@@ -196,6 +210,27 @@ void x86_restore_host_spec_ctrl(u64 curr
 }
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_restore_host);
 
+/*
+ * A condition that may affect the SPEC_CTRL MSR has changed.
+ * Recalculate a new value for this CPU and set it.
+ *
+ * It is not easy to optimize the wrmsrl() away unless the
+ * callers have a full understanding of all the conditions
+ * that affect the output of x86_calculate_kernel_spec_ctrl().
+ *
+ * Try not to call this too often.
+ */
+void x86_sync_spec_ctrl(void)
+{
+	u64 new_spec_ctrl = x86_calculate_kernel_spec_ctrl();
+
+	if (!boot_cpu_has(X86_FEATURE_SPEC_CTRL))
+		return;
+
+	wrmsrl(MSR_IA32_SPEC_CTRL, new_spec_ctrl);
+}
+EXPORT_SYMBOL_GPL(x86_sync_spec_ctrl);
+
 static void x86_amd_rds_enable(void)
 {
 	u64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_rds_mask;
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -353,13 +353,11 @@ DECLARE_PER_CPU(unsigned int, bpf_prog_r
 
 static inline void bpf_enter_prog(const struct bpf_prog *fp)
 {
-	int *count = &get_cpu_var(bpf_prog_ran);
-	(*count)++;
 	/*
 	 * Upon the first entry to BPF code, we need to reduce
 	 * memory speculation to mitigate attacks targeting it.
 	 */
-	if (*count == 1)
+	if (this_cpu_inc_return(bpf_prog_ran) == 1)
 		cpu_enter_reduced_memory_speculation();
 }
 
--- a/include/linux/nospec.h
+++ b/include/linux/nospec.h
@@ -72,6 +72,8 @@ static inline void cpu_enter_reduced_mem
 static inline void cpu_leave_reduced_memory_speculation(void)
 {
 }
+#else
+#include <asm/rmspec.h>
 #endif
 
 #endif /* _LINUX_NOSPEC_H */
