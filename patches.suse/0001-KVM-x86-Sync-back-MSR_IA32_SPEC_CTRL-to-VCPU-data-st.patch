From: Joerg Roedel <jroedel@suse.de>
Date: Tue, 12 Jun 2018 15:09:29 +0200
Subject: KVM: x86: Sync back MSR_IA32_SPEC_CTRL to VCPU data structure
Patch-mainline: no, different implementation
References: bsc#1096242, bsc#1096281

KVM guests get full access to the SPEC_CTRL MSR when they
run, so we need to make sure that any value a guest writes
there does not leak into the host.

So sync the MSR value back to {svm|vmx}->spec_ctrl after
every #vmexit and make sure we reset the MSR even when the
host has disabled any spectre mitigations.

Signed-off-by: Joerg Roedel <jroedel@suse.de>
---
 arch/x86/include/asm/cpufeature.h | 1 +
 arch/x86/kernel/cpu/bugs.c        | 6 ------
 arch/x86/kernel/cpu/scattered.c   | 1 +
 arch/x86/kvm/svm.c                | 6 ++++--
 arch/x86/kvm/vmx.c                | 6 ++++--
 5 files changed, 10 insertions(+), 10 deletions(-)

diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index d925948..872b9d7 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -185,6 +185,7 @@
 #define X86_FEATURE_SPEC_CTRL	( 7*32+20) /* Control Speculation Control */
 #define X86_FEATURE_IBRS	( 7*32+21) /* "" Indirect Branch Restricted Speculation */
 #define X86_FEATURE_SSBD         ( 7*32+22) /* Speculative Store Bypass Disable */
+#define X86_FEATURE_SPEC_CTRL_MSR	( 7*32+23) /* "" Speculation Control MSR */
 
 #define X86_FEATURE_AMD_SSBD     ( 7*32+27)  /* "" AMD SSBD implementation */
 #define X86_FEATURE_SPEC_STORE_BYPASS_DISABLE  ( 7*32+28) /* "" Disable Speculative Store Bypass. */
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index f0eaa24..764d524 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -793,9 +793,6 @@ void x86_spec_ctrl_set_guest(u64 guest_spec_ctrl)
 {
 	u64 host = x86_spec_ctrl_base;
 
-	if (!boot_cpu_has(X86_FEATURE_IBRS))
-		return;
-
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
 
@@ -808,9 +805,6 @@ void x86_spec_ctrl_restore_host(u64 guest_spec_ctrl)
 {
 	u64 host = x86_spec_ctrl_base;
 
-	if (!boot_cpu_has(X86_FEATURE_IBRS))
-		return;
-
 	if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)
 		host |= ssbd_tif_to_spec_ctrl(current_thread_info()->flags);
 
diff --git a/arch/x86/kernel/cpu/scattered.c b/arch/x86/kernel/cpu/scattered.c
index d38ebc8..7f1e02b 100644
--- a/arch/x86/kernel/cpu/scattered.c
+++ b/arch/x86/kernel/cpu/scattered.c
@@ -39,6 +39,7 @@ void __cpuinit init_scattered_cpuid_features(struct cpuinfo_x86 *c)
 		{ X86_FEATURE_APERFMPERF,	CR_ECX, 0, 0x00000006, 0 },
 		{ X86_FEATURE_EPB,		CR_ECX, 3, 0x00000006, 0 },
 		{ X86_FEATURE_SPEC_CTRL,        CR_EDX,26, 0x00000007, 0 },
+		{ X86_FEATURE_SPEC_CTRL_MSR,    CR_EDX,26, 0x00000007, 0 },
 		{ X86_FEATURE_SSBD,		CR_EDX,31, 0x00000007, 0 },
 		{ X86_FEATURE_XSAVEOPT,		CR_EAX,	0, 0x0000000d, 1 },
 		{ X86_FEATURE_CPB,		CR_EDX, 9, 0x80000007, 0 },
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index b605634..42dc5ff 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -3832,7 +3832,7 @@ static void svm_vcpu_run(struct kvm_vcpu *vcpu)
 
 	local_irq_enable();
 
-	if (x86_ibrs_enabled())
+	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_MSR))
 		x86_spec_ctrl_set_guest(svm->spec_ctrl);
 
 	asm volatile (
@@ -3936,8 +3936,10 @@ static void svm_vcpu_run(struct kvm_vcpu *vcpu)
 
 	reload_tss(vcpu);
 
-	if (x86_ibrs_enabled())
+	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_MSR)) {
+		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 		x86_spec_ctrl_restore_host(svm->spec_ctrl);
+	}
 
 	local_irq_disable();
 
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index 476a7d6..ad37756 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -6748,7 +6748,7 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 	if (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)
 		vmx_set_interrupt_shadow(vcpu, 0);
 
-	if (x86_ibrs_enabled())
+	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_MSR))
 		x86_spec_ctrl_set_guest(vmx->spec_ctrl);
 
 	vmx->__launched = vmx->loaded_vmcs->launched;
@@ -6851,8 +6851,10 @@ static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
 #endif
 	      );
 
-	if (x86_ibrs_enabled())
+	if (static_cpu_has(X86_FEATURE_SPEC_CTRL_MSR)) {
+		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 		x86_spec_ctrl_restore_host(vmx->spec_ctrl);
+	}
 
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
-- 
2.12.3

