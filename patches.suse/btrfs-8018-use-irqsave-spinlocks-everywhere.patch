From: David Sterba <dsterba@suse.cz>
Date: Thu Dec  8 03:32:58 CET 2011
Patch-mainline: no, workaround
References: FATE#306586 bnc#734522
Subject: [PATCH] btrfs: use irqsave spinlocks everywhere

Workaround for rcu stall when timer interrupt fires but is not able to continue
for yet unknown reasons.
All _irq variants are changed to _irqsave.

Signed-off-by: David Sterba <dsterba@suse.cz>
---
 fs/btrfs/async-thread.c     |   45 +++++++++++++++++++++++++-------------------
 fs/btrfs/extent_io.c        |    5 ++--
 fs/btrfs/free-space-cache.c |    5 ++--
 3 files changed, 32 insertions(+), 23 deletions(-)

--- a/fs/btrfs/async-thread.c
+++ b/fs/btrfs/async-thread.c
@@ -233,8 +233,9 @@ static void put_worker(struct btrfs_work
 static int try_worker_shutdown(struct btrfs_worker_thread *worker)
 {
 	int freeit = 0;
+	unsigned long flags;
 
-	spin_lock_irq(&worker->lock);
+	spin_lock_irqsave(&worker->lock, flags);
 	spin_lock(&worker->workers->lock);
 	if (worker->workers->num_workers > 1 &&
 	    worker->idle &&
@@ -248,7 +249,7 @@ static int try_worker_shutdown(struct bt
 		worker->workers->num_workers--;
 	}
 	spin_unlock(&worker->workers->lock);
-	spin_unlock_irq(&worker->lock);
+	spin_unlock_irqrestore(&worker->lock, flags);
 
 	if (freeit)
 		put_worker(worker);
@@ -261,6 +262,7 @@ static struct btrfs_work *get_next_work(
 {
 	struct btrfs_work *work = NULL;
 	struct list_head *cur = NULL;
+	unsigned long flags;
 
 	if(!list_empty(prio_head)) {
 		cur = prio_head->next;
@@ -277,7 +279,7 @@ static struct btrfs_work *get_next_work(
 	}
 
 refill:
-	spin_lock_irq(&worker->lock);
+	spin_lock_irqsave(&worker->lock, flags);
 	list_splice_tail_init(&worker->prio_pending, prio_head);
 	list_splice_tail_init(&worker->pending, head);
 
@@ -285,7 +287,7 @@ refill:
 		cur = prio_head->next;
 	else if (!list_empty(head))
 		cur = head->next;
-	spin_unlock_irq(&worker->lock);
+	spin_unlock_irqrestore(&worker->lock, flags);
 
 	if (!cur)
 		goto out_fail;
@@ -306,6 +308,7 @@ static int worker_loop(void *arg)
 	struct list_head head;
 	struct list_head prio_head;
 	struct btrfs_work *work;
+	unsigned long flags;
 
 	INIT_LIST_HEAD(&head);
 	INIT_LIST_HEAD(&prio_head);
@@ -337,15 +340,15 @@ again:
 			cond_resched();
 		}
 
-		spin_lock_irq(&worker->lock);
+		spin_lock_irqsave(&worker->lock, flags);
 		check_idle_worker(worker);
 
 		if (freezing(current)) {
 			worker->working = 0;
-			spin_unlock_irq(&worker->lock);
+			spin_unlock_irqrestore(&worker->lock, flags);
 			refrigerator();
 		} else {
-			spin_unlock_irq(&worker->lock);
+			spin_unlock_irqrestore(&worker->lock, flags);
 			if (!kthread_should_stop()) {
 				cpu_relax();
 				/*
@@ -375,7 +378,7 @@ again:
 					break;
 
 				/* still no more work?, sleep for real */
-				spin_lock_irq(&worker->lock);
+				spin_lock_irqsave(&worker->lock, flags);
 				set_current_state(TASK_INTERRUPTIBLE);
 				if (!list_empty(&worker->pending) ||
 				    !list_empty(&worker->prio_pending)) {
@@ -389,7 +392,7 @@ again:
 				 * adds something new to the queue
 				 */
 				worker->working = 0;
-				spin_unlock_irq(&worker->lock);
+				spin_unlock_irqrestore(&worker->lock, flags);
 
 				if (!kthread_should_stop()) {
 					schedule_timeout(HZ * 120);
@@ -413,8 +416,9 @@ void btrfs_stop_workers(struct btrfs_wor
 	struct list_head *cur;
 	struct btrfs_worker_thread *worker;
 	int can_stop;
+	unsigned long flags;
 
-	spin_lock_irq(&workers->lock);
+	spin_lock_irqsave(&workers->lock, flags);
 	workers->stopping = 1;
 	list_splice_init(&workers->idle_list, &workers->worker_list);
 	while (!list_empty(&workers->worker_list)) {
@@ -430,13 +434,13 @@ void btrfs_stop_workers(struct btrfs_wor
 			can_stop = 1;
 		} else
 			can_stop = 0;
-		spin_unlock_irq(&workers->lock);
+		spin_unlock_irqrestore(&workers->lock, flags);
 		if (can_stop)
 			kthread_stop(worker->task);
-		spin_lock_irq(&workers->lock);
+		spin_lock_irqsave(&workers->lock, flags);
 		put_worker(worker);
 	}
-	spin_unlock_irq(&workers->lock);
+	spin_unlock_irqrestore(&workers->lock, flags);
 }
 
 /*
@@ -470,6 +474,7 @@ static int __btrfs_start_workers(struct
 {
 	struct btrfs_worker_thread *worker;
 	int ret = 0;
+	unsigned long flags;
 
 	worker = kzalloc(sizeof(*worker), GFP_NOFS);
 	if (!worker) {
@@ -493,7 +498,7 @@ static int __btrfs_start_workers(struct
 		goto fail;
 	}
 
-	spin_lock_irq(&workers->lock);
+	spin_lock_irqsave(&workers->lock, flags);
 	if (workers->stopping) {
 		spin_unlock_irq(&workers->lock);
 		goto fail_kthread;
@@ -503,7 +508,7 @@ static int __btrfs_start_workers(struct
 	workers->num_workers++;
 	workers->num_workers_starting--;
 	WARN_ON(workers->num_workers_starting < 0);
-	spin_unlock_irq(&workers->lock);
+	spin_unlock_irqrestore(&workers->lock, flags);
 
 	wake_up_process(worker->task);
 	return 0;
@@ -512,17 +517,19 @@ fail_kthread:
 	kthread_stop(worker->task);
 fail:
 	kfree(worker);
-	spin_lock_irq(&workers->lock);
+	spin_lock_irqsave(&workers->lock, flags);
 	workers->num_workers_starting--;
-	spin_unlock_irq(&workers->lock);
+	spin_unlock_irqrestore(&workers->lock, flags);
 	return ret;
 }
 
 int btrfs_start_workers(struct btrfs_workers *workers)
 {
-	spin_lock_irq(&workers->lock);
+	unsigned long flags;
+
+	spin_lock_irqsave(&workers->lock, flags);
 	workers->num_workers_starting++;
-	spin_unlock_irq(&workers->lock);
+	spin_unlock_irqrestore(&workers->lock, flags);
 	return __btrfs_start_workers(workers);
 }
 
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -4551,6 +4551,7 @@ void clear_extent_buffer_dirty(struct ex
 	unsigned long i;
 	unsigned long num_pages;
 	struct page *page;
+	unsigned long flags;
 
 	num_pages = num_extent_pages(eb->start, eb->len);
 	WARN_ON(atomic_read(&eb->refs) == 0);
@@ -4564,13 +4565,13 @@ void clear_extent_buffer_dirty(struct ex
 		WARN_ON(!PagePrivate(page));
 
 		clear_page_dirty_for_io(page);
-		spin_lock_irq(&page->mapping->tree_lock);
+		spin_lock_irqsave(&page->mapping->tree_lock, flags);
 		if (!PageDirty(page)) {
 			radix_tree_tag_clear(&page->mapping->page_tree,
 						page_index(page),
 						PAGECACHE_TAG_DIRTY);
 		}
-		spin_unlock_irq(&page->mapping->tree_lock);
+		spin_unlock_irqrestore(&page->mapping->tree_lock, flags);
 		ClearPageError(page);
 		unlock_page(page);
 	}
--- a/fs/btrfs/free-space-cache.c
+++ b/fs/btrfs/free-space-cache.c
@@ -2104,10 +2104,11 @@ u64 btrfs_find_space_for_alloc(struct bt
 {
 	struct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;
 	struct btrfs_free_space *entry = NULL;
+	unsigned long flags;
 	u64 bytes_search = bytes + empty_size;
 	u64 ret = 0;
 
-	spin_lock_irq(&ctl->tree_lock);
+	spin_lock_irqsave(&ctl->tree_lock, flags);
 	entry = find_free_space(ctl, &offset, &bytes_search);
 	if (!entry)
 		goto out;
@@ -2128,7 +2129,7 @@ u64 btrfs_find_space_for_alloc(struct bt
 	}
 
 out:
-	spin_unlock_irq(&ctl->tree_lock);
+	spin_unlock_irqrestore(&ctl->tree_lock, flags);
 
 	return ret;
 }
