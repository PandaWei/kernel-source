From b5122a6c3822880fee8f3dd871723f3d9b860425 Mon Sep 17 00:00:00 2001
From: Thomas Gleixner <tglx@linutronix.de>
Date: Sun, 29 Apr 2018 15:21:42 +0200
Subject: xen/x86/process: Allow runtime control of Speculative Store
 Bypass
Patch-mainline: Never, SUSE-Xen specific
References: bsc#1087082 CVE-2018-3639

The Speculative Store Bypass vulnerability can be mitigated with the
Reduced Data Speculation (RDS) feature. To allow finer grained control of
this eventually expensive mitigation a per task mitigation control is
required.

Add a new TIF_RDS flag and put it into the group of TIF flags which are
evaluated for mismatch in switch_to(). If these bits differ in the previous
and the next task, then the slow path function __switch_to_xtra() is
invoked. Implement the TIF_RDS dependent mitigation control in the slow
path.

If the prctl for controlling Speculative Store Bypass is disabled or no
task uses the prctl then there is no overhead in the switch_to() fast
path.

Update the KVM related speculation control functions to take TID_RDS into
account as well.

Based on a patch from Tim Chen. Completely rewritten.

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Reviewed-by: Ingo Molnar <mingo@kernel.org>
Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Acked-by: Joerg Roedel <jroedel@suse.de>
Automatically created from "patches.arch/0013-x86-process-Allow-runtime-control-of-Speculative-Sto.patch" by xen-port-patches.py

--- a/arch/x86/kernel/process-xen.c
+++ b/arch/x86/kernel/process-xen.c
@@ -23,6 +23,7 @@
 #include <asm/i387.h>
 #include <asm/debugreg.h>
 #include <asm/spec_ctrl.h>
+#include <asm/spec-ctrl.h>
 #include <xen/evtchn.h>
 
 struct kmem_cache *task_xstate_cachep;
@@ -193,6 +194,36 @@ int set_tsc_mode(unsigned int val)
 	return 0;
 }
 
+static __always_inline void __speculative_store_bypass_update(int rds)
+{
+	u64 msr;
+
+	if (static_cpu_has(X86_FEATURE_AMD_RDS)) {
+		msr = x86_amd_ls_cfg_base | rds_tif_to_amd_ls_cfg(rds);
+#ifdef CONFIG_XEN
+		/*
+		 * At the moment Xen does not virtualize LS_CFG, and it
+		 * unconditionally sets the flag in question (unless disabled).
+		 * Avoid the MSR write when possible, as it triggers a (rate
+		 * limited) hypervisor log message. (This could be further
+		 * enhanced by also avoiding the write if the bit is fixed to
+		 * zero, but that would be more involved. If any guest is to
+		 * rely on the feature, Xen better had it enabled globally.)
+		 */
+		if (!(x86_amd_ls_cfg_base & x86_amd_ls_cfg_rds_mask))
+#endif
+		wrmsrl(MSR_AMD64_LS_CFG, msr);
+	} else {
+		msr = x86_spec_ctrl_base | rds_tif_to_spec_ctrl(rds);
+		wrmsrl(MSR_IA32_SPEC_CTRL, msr);
+	}
+}
+
+void speculative_store_bypass_update(void)
+{
+	__speculative_store_bypass_update(current_thread_info()->flags);
+}
+
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)
 {
 	struct thread_struct *prev, *next;
@@ -219,6 +250,11 @@ void __switch_to_xtra(struct task_struct
 		else
 			hard_enable_TSC();
 	}
+
+	if (test_tsk_thread_flag(prev_p, TIF_RDS) ^
+	    test_tsk_thread_flag(next_p, TIF_RDS))
+		__speculative_store_bypass_update(test_tsk_thread_flag(next_p, TIF_RDS));
+
 	propagate_user_return_notify(prev_p, next_p);
 }
 
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -155,7 +155,7 @@ struct thread_info {
 	(_TIF_IO_BITMAP|_TIF_NOTSC|_TIF_BLOCKSTEP|_TIF_SSBD)
 
 #else
-#define _TIF_WORK_CTXSW (_TIF_NOTSC /*todo | _TIF_BLOCKSTEP */)
+#define _TIF_WORK_CTXSW (_TIF_NOTSC /*todo | _TIF_BLOCKSTEP */ | _TIF_RDS)
 #endif
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW|_TIF_DEBUG)
